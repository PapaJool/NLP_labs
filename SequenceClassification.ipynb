{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6766,"databundleVersionId":108386,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ntrain_data = pd.read_json('/kaggle/input/sentiment-analysis-in-russian/train.json')\ntrain_data['sentiment'] = train_data['sentiment'].map({'positive': 0, 'neutral': 1, 'negative': 2})\ntrain_data, test_data = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T08:28:38.220992Z","iopub.execute_input":"2024-03-20T08:28:38.221350Z","iopub.status.idle":"2024-03-20T08:28:40.510069Z","shell.execute_reply.started":"2024-03-20T08:28:38.221314Z","shell.execute_reply":"2024-03-20T08:28:40.509255Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:28:40.680830Z","iopub.execute_input":"2024-03-20T08:28:40.681621Z","iopub.status.idle":"2024-03-20T08:28:54.048744Z","shell.execute_reply.started":"2024-03-20T08:28:40.681592Z","shell.execute_reply":"2024-03-20T08:28:54.047778Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoTokenizer, AutoModelForPreTraining\n\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n\nmodel = AutoModelForPreTraining.from_pretrained(\"cointegrated/rubert-tiny2\")\n\ntrain_input_ids = []\ntrain_attention_masks = []\n\nfor text in train_data['text']:\n    encoding = tokenizer.encode_plus(text, \n                         add_special_tokens=True, \n                         max_length=1024, \n                         padding='max_length', \n                         truncation=True, \n                         return_attention_mask=True,\n                         return_tensors='pt')\n    \n    train_input_ids.append(encoding['input_ids'])\n    \n    train_attention_masks.append(encoding['attention_mask'])\n\ntrain_input_ids = torch.cat(train_input_ids, dim = 0)\ntrain_attention_masks = torch.cat(train_attention_masks, dim = 0)\n\ntest_input_ids = []\ntest_attention_masks = []\n\nfor text in test_data['text']:\n    encoding = tokenizer.encode_plus(text, \n                         add_special_tokens=True, \n                         max_length=1024, \n                         padding='max_length', \n                         truncation=True, \n                         return_attention_mask=True,\n                         return_tensors='pt')\n    \n    test_input_ids.append(encoding['input_ids'])\n    \n    test_attention_masks.append(encoding['attention_mask'])\n\ntest_input_ids = torch.cat(test_input_ids, dim = 0)\ntest_attention_masks = torch.cat(test_attention_masks, dim = 0)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:28:54.050579Z","iopub.execute_input":"2024-03-20T08:28:54.050886Z","iopub.status.idle":"2024-03-20T08:29:41.650480Z","shell.execute_reply.started":"2024-03-20T08:28:54.050852Z","shell.execute_reply":"2024-03-20T08:29:41.649440Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8fc22146a254a1ca2cbb31b12ee592b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1be49f96dd4ecdb7e0fe45b4b53083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad06e551b3347429bf3fb1a69dfcc1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cc2b8b9a76446cab9463f0d798fc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e40739472a4b8d8e5a0e665a0eb696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9f70599e134304a6ee9bee13709c7b"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\ntrain_labels = torch.tensor(train_data.sentiment.values)\ntest_labels = torch.tensor(test_data.sentiment.values)\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n\nbatch_size = 16\n\ntrain_loader = DataLoader(\n            train_dataset, \n            sampler = RandomSampler(train_dataset), \n            batch_size = batch_size \n        )\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:29:41.654736Z","iopub.execute_input":"2024-03-20T08:29:41.655229Z","iopub.status.idle":"2024-03-20T08:29:41.666265Z","shell.execute_reply.started":"2024-03-20T08:29:41.655200Z","shell.execute_reply":"2024-03-20T08:29:41.665340Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\ndef train_cls(model, train_loader, optimizer):\n    model.train()\n    total_loss = 0\n    train_preds = []\n    train_labels = []\n\n    for input_ids, attention_mask, labels in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        train_preds.extend(torch.argmax(logits, dim=1).tolist())\n        train_labels.extend(labels.tolist())\n\n    train_accuracy = accuracy_score(train_labels, train_preds)\n    train_loss = total_loss / len(train_loader)\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n    return train_loss, train_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:31:59.624748Z","iopub.execute_input":"2024-03-20T08:31:59.625123Z","iopub.status.idle":"2024-03-20T08:31:59.633618Z","shell.execute_reply.started":"2024-03-20T08:31:59.625094Z","shell.execute_reply":"2024-03-20T08:31:59.632554Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def val_cls(model, val_loader):\n    model.eval()\n    val_preds = []\n    val_labels = []\n    val_loss = 0\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in tqdm(val_loader):\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            val_loss += loss.item()\n\n            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n            val_labels.extend(labels.tolist())\n    val_accuracy = accuracy_score(val_labels, val_preds)\n    val_loss /= len(val_loader)\n    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:32:02.604010Z","iopub.execute_input":"2024-03-20T08:32:02.604612Z","iopub.status.idle":"2024-03-20T08:32:02.612168Z","shell.execute_reply.started":"2024-03-20T08:32:02.604584Z","shell.execute_reply":"2024-03-20T08:32:02.611145Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:29:41.668942Z","iopub.execute_input":"2024-03-20T08:29:41.669298Z","iopub.status.idle":"2024-03-20T08:29:41.705688Z","shell.execute_reply.started":"2024-03-20T08:29:41.669248Z","shell.execute_reply":"2024-03-20T08:29:41.704568Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"cointegrated/rubert-tiny2\", \n    num_labels = 3,  \n    output_hidden_states = False, \n)\nopt = torch.optim.AdamW(model.parameters(), lr=5e-5)\nmodel.cuda()\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    _,cls_acc = train_cls(model, train_loader, opt)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:49:23.848177Z","iopub.execute_input":"2024-03-20T07:49:23.848557Z","iopub.status.idle":"2024-03-20T07:54:01.699918Z","shell.execute_reply.started":"2024-03-20T07:49:23.848527Z","shell.execute_reply":"2024-03-20T07:54:01.699087Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 414/414 [01:32<00:00,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8109, Train Accuracy: 0.6139\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414/414 [01:32<00:00,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6067, Train Accuracy: 0.7352\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414/414 [01:32<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4194, Train Accuracy: 0.8304\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"_,val_cls_acc = val_cls(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:54:49.919852Z","iopub.execute_input":"2024-03-20T07:54:49.920240Z","iopub.status.idle":"2024-03-20T07:54:57.529756Z","shell.execute_reply.started":"2024-03-20T07:54:49.920207Z","shell.execute_reply":"2024-03-20T07:54:57.528860Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 104/104 [00:07<00:00, 13.69it/s]","output_type":"stream"},{"name":"stdout","text":"       Val Loss: 0.7059, Val Accuracy: 0.7042\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"val_cls_acc","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:55:05.554887Z","iopub.execute_input":"2024-03-20T07:55:05.555695Z","iopub.status.idle":"2024-03-20T07:55:05.561299Z","shell.execute_reply.started":"2024-03-20T07:55:05.555663Z","shell.execute_reply":"2024-03-20T07:55:05.560413Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.7041742286751361"},"metadata":{}}]},{"cell_type":"code","source":"def train_mean(model, train_loader, opt):\n    train_preds = []\n    train_labels = []\n    model.train()\n    for input_ids, attention_mask, labels in tqdm(train_loader):\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        outputs = model(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        labels=labels,\n                        return_dict=True)\n\n        mean = torch.mean(outputs.hidden_states[-1][:, 1:], dim=1)\n        logits = model.classifier(mean)\n        loss = torch.nn.functional.cross_entropy(logits, labels)\n        loss.backward()\n\n        opt.step()\n        opt.zero_grad()\n\n        logits = outputs.logits\n        train_preds.extend(torch.argmax(logits, dim=-1).tolist())\n        train_labels.extend(labels.tolist())\n        train_accuracy = accuracy_score(train_labels, train_preds)\n    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n        \n    return train_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:29:41.706831Z","iopub.execute_input":"2024-03-20T08:29:41.707147Z","iopub.status.idle":"2024-03-20T08:29:41.719529Z","shell.execute_reply.started":"2024-03-20T08:29:41.707099Z","shell.execute_reply":"2024-03-20T08:29:41.718486Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def eval_mean(model, test_loader):\n    val_preds = []\n    val_labels = []\n    model.eval()\n    for input_ids, attention_mask, labels in tqdm(test_loader):\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            labels=labels,\n                            return_dict=True)\n            mean = torch.mean(outputs.hidden_states[-1][:, 1:], dim=1)\n            logits = model.classifier(mean)\n        predictions = torch.argmax(logits, dim=-1)\n        val_preds.extend(torch.argmax(logits, dim=-1).tolist())\n        val_labels.extend(labels.tolist())\n        val_accuracy = accuracy_score(val_labels, val_preds)\n\n    return val_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:29:41.720846Z","iopub.execute_input":"2024-03-20T08:29:41.721623Z","iopub.status.idle":"2024-03-20T08:29:41.733498Z","shell.execute_reply.started":"2024-03-20T08:29:41.721584Z","shell.execute_reply":"2024-03-20T08:29:41.732526Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\n# Определение модели, оптимизатора и других параметров\nmodel = BertForSequenceClassification.from_pretrained(\n    \"cointegrated/rubert-tiny2\", \n    num_labels=3,\n    output_hidden_states=True\n)\nmodel.cuda()\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Запуск обучения и валидации\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    train_accuracy = train_mean(model, train_loader, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:32:11.431212Z","iopub.execute_input":"2024-03-20T08:32:11.431971Z","iopub.status.idle":"2024-03-20T08:36:51.786958Z","shell.execute_reply.started":"2024-03-20T08:32:11.431936Z","shell.execute_reply":"2024-03-20T08:36:51.785933Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 414/414 [01:33<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 0.6083\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414/414 [01:33<00:00,  4.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 0.7315\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 414/414 [01:32<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 0.8440\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_mean(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:36:56.165642Z","iopub.execute_input":"2024-03-20T08:36:56.166449Z","iopub.status.idle":"2024-03-20T08:37:03.862807Z","shell.execute_reply.started":"2024-03-20T08:36:56.166418Z","shell.execute_reply":"2024-03-20T08:37:03.861946Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 104/104 [00:07<00:00, 13.53it/s]\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0.7011494252873564"},"metadata":{}}]}]}
